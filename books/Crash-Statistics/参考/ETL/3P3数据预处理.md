# 数据分析第一步：数据的预处理

今天开始，我们正式进入统计学的一大分支——描述统计。在最开头，我们不是上来就讲怎么分析数据，我们应该先讲下数据预处理。

很多人会疑惑做一个Project或者写一篇Paper，最难的是什么？要我讲啊，最难的是数据。数据收集完成，项目完成了50%。而数据收集完成之后，很多人就会马上开始进行数据处理和分析，事实上这是不对的。

因为你不清楚你的数据是否有问题（什么问题都有可能，会导致你的分析出现各种问题）。

所以你拿到数据后的第一步，应该是对数据做预处理，或者用大数据时代的话——叫数据清洗或者ETL（Extract-Transform-Load），我想预处理还会占掉Project花费时间的20%吧。

## 数据预处理的手段

数据的预处理是在对数据分类或分组之前所做的必要处理，其目的在于发现数据的异常值并处理掉，其手段包括数据的审核、筛选、排序等。具体可分为：

* 数据审核:包括直接数据的完整性审核以及准确性审核（是否客观），间接数据的适用性审核以及时效性审核
* 数据筛选:数据筛选，就是对于数据里面的异常值（存在错误，不符合调查要求等），在现在来说就是dirty data（脏数据），将这些数据剔除
* 数据排序:事实上数据排序更多的目的还是为了更方便地发现异常值，是做数据清洗的手段
* 数据透视:借鉴于Excel里的数据透视表，事实上就是数据的重铸，融合和汇总，从而得到我们需要的数据

在这个阶段，不喜编程的同学推荐用Excel来做数据预处理（通过数据透视图、替换数据、排序、Countif等工具和Excel函数高效完成预处理），更高级的一般可以考虑用R、Python等编程语言进行清洗预处理，或者像在数据库里用SQL语句也是可以的。

那么接下来一个个的介绍预处理的内容。

## 数据预处理手段——(1)数据审核

数据审核就是检查数据中是否有错误。对于通过调查取得的原始数据(raw data)，主要从完整性和准确性两个方面去审核。完整性审核主要是检查应调查的单位或个体是否有遗漏，所有的调查项目是否填写齐全等。准确性审核主要是检查数据是否有错误，是否存在异常值等。对于异常值要仔细鉴别：如果异常值属于记录时的错误，在分析之前应予以纠正；如果异常值是一个正确的值，则应予以保留。

对于通过其他渠道取得的二手数据，应着重审核数据的适用性和时效性。二手数据可 以来自多种渠道，有些数据可能是为特定目的通过专门调查而取得的，或者是已经按特定目的的需要做了加工整理。对于使用者来说，首先应弄清楚数据的来源、数据的口径以及有关的背景材料，以便确定这些数据是否符合分析研究的需要，不能盲目生搬硬套。此外，还要对数据的时效性进行审核，对千时效性较强的问题，如果所取得的数据过于滞后，就可能失去研究的意义。

## 数据预处理手段——(2)数据筛选

数据筛选(data filter)是根据需要找出符合特定条件的某类数据。比如，找出销售额在1000万元以上的企业；找出考试成绩在90分以上的学生；等等。数据筛选可借助Pyhton自动完成。

这里啊，我列举一下Python筛选数据的代码：

## 数据预处理手段——(3)数据排序

数据排序是指按一定顺序将数据排列，以便研究者通过浏览数据发现一些明显的特征或趋势，找到解决问题的线索。除此之外，排序还有助于对数据进行检查纠错，以及为重新归类或分组等提供方便。在某些场合，排序本身就是分析的目的之一，例如了解究竟谁是中国汽车生产的三巨头，对于汽车生产厂商而言，不论它是伙伴还是竞争者，都是很有用的信息。美国的《财富》杂志每年都要在全世界范围内排出500强企业，通过这一信息，不仅可以了解自己企业所处的位置，清楚自己的差距，还可以从一个侧面了解到竞争对手的状况，有效制定企业的发展规划和战略目标。

对于分类数据，如果是字母型数据，排序则有升序、降序之分，但习惯上升序用得更多，因为升序与字母的自然排列相同；如果是汉字型数据，排序方式则很多，比如按汉字的首个拼音字母排列，这与字母型数据的排序完全一样，也可按姓氏笔画排序，其中也有笔画多少的升序、降序之分。对于数值型数据，排序只有两种，即递增和递减。排序均可借助Python很容易地完成。

## 数据预处理手段——(4)数据透视表

为了从复杂的数据中提取有用的信息，可以利用Python提供的【数据透视表】 (pivottable)工具。利用数据透视表，  可以对数据表的重要信息按使用者的习惯或分析要求进行汇总和作图，形成一个符合需要的交叉表（列联表）。注意：在利用数据透视表时，数据源表中的首行必须有列标题。

学习数据透视表最好的办法是多练习。



### 1.数据的预处理
本章正式进入统计学的一大分支——描述统计。
很多人会疑惑做一个Project或者写一篇Paper，最难的是什么？我曾经不止一次说过，最难的是数据。数据收集完成，项目完成了50%。而数据收集完成之后，很多人就会马上开始进行数据处理和分析，事实上这是不对的。

因为你不清楚你的数据是否有问题（什么问题都有可能，会导致你的分析出现各种问题）。

所以你拿到数据后的第一步，应该是对数据做预处理，或者用大数据时代的话——叫数据清洗或者ETL（Extract-Transform-Load），我想预处理还会占掉Project花费时间的20%吧。

那么接下来先介绍下预处理的内容。
数据预处理：
> * 数据审核
> * 数据筛选
> * 数据排序
> * 数据透视

数据审核，包括直接数据的完整性审核以及准确性审核（是否客观），间接数据的适用性审核以及时效性审核；数据筛选，就是对于数据里面的异常值（存在错误，不符合调查要求等），在现在来说就是dirty data（脏数据），将这些数据剔除；数据排序，事实上数据排序更多的目的还是为了更方便地发现异常值，是做数据清洗的手段；数据透视，借鉴于Excel里的数据透视表，事实上就是数据的重铸，融合和汇总，从而得到我们需要的数据。

总的来说，前期预处理需要对数据进行排序、汇总和观察发现相关的数据异常值等。在这个阶段，不喜编程的同学推荐用Excel来做数据预处理（通过数据透视图、替换数据、排序、Countif等工具和Excel函数高效完成预处理），更高级的一般可以考虑用R、Python等编程语言进行清洗预处理，或者像在数据库里用SQL语句也是可以的。

响应一下本部分的标题，R语言实现，交代几个简单的语句进行数据清洗。

```R    
    #x为数据框、数组或矩阵，通过summary可以获取平均值、中位数、四分位数等，如果有缺失数据，则会显示NAN等。
    summary(x)
    
    #表示y是按照x的第一行先升序排列，然后再按x的第二列降序排列得到的数据，-表示降序。
    y<-x[order(x[1],-x[2)]
    
    #去除NA所在行和列
    y<-na.omit(x)
```

