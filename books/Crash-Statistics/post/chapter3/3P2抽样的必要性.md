
### 抽样的原因：总体通常是是无限的

大数据不能被直接拿来使用，统计学依然是数据分析的灵魂

现在社会上有一种流行的说法，认为在大数据时代，“样本 = 全体”，人们得到的不是抽样数据而是全数据，因而只需要简单地数一数就可以下结论了，复杂的统计学方法可以不再需要了。

在我看来，这种观点非常错误。首先，大数据告知信息但不解释信息。打个比方说，大数据是“原油”而不是“汽油”，不能被直接拿来使用。就像股票市场，即使把所有的数据都公布出来，不懂的人依然不知道数据代表的信息。大数据时代，统计学依然是数据分析的灵魂。正如加州大学伯克利分校迈克尔·乔丹教授指出的：没有系统的数据科学作为指导的大数据研究，就如同不利用工程科学的知识来建造桥梁，很多桥梁可能会坍塌，并带来严重的后果。

其次，全数据的概念本身很难经得起推敲。全数据，顾名思义就是全部数据。这在某些特定的场合对于某些特定的问题确实可能实现。比如，要比较清华、北大两校同学数学能力整体上哪个更强，可以收集到两校同学高考时的数学成绩作为研究的数据对象。从某种意义上说，这是全数据。但是，并不是说我们有了这个全数据就能很好地回答问题。

一方面，这个数据虽然是全数据，但仍然具有不确定性。入校时的数学成绩并不一定完全代表学生的数学能力。假如让所有同学重新参加一次高考，几乎每个同学都会有一个新的成绩。分别用这两组全数据去做分析，结论就可能发生变化。另一方面，事物在不断地发展和变化，同学入校时的成绩并不能够代表现在的能力。全体同学的高考成绩数据，仅对于那次考试而言是全数据。“全”是有边界的，超出了边界就不再是全知全能了。事物的发展充满了不确定性，而统计学，既研究如何从数据中把信息和规律提取出来，找出最优化的方案；也研究如何把数据当中的不确定性量化出来。

所以说，在大数据时代，数据分析的很多根本性问题和小数据时代并没有本质区别。当然，大数据的特点，确实对数据分析提出了全新挑战。例如，许多传统统计方法应用到大数据上，巨大计算量和存储量往往使其难以承受；对结构复杂、来源多样的数据，如何建立有效的统计学模型也需要新的探索和尝试。对于新时代的数据科学而言，这些挑战也同时意味着巨大的机遇，有可能会产生新的思想、方法和技术。


当总体中所含个体总数是有限时，称为有限总体，否则，称为无限总体。

在实际中全面了解总体的情况，往往难以办到。例如考察某厂生产的灯泡的使用寿命，该厂生产的所有灯泡的使用寿命为总体，每个灯泡的使用寿命为一个个体。我们不可能对所有灯泡进行试验，记录每一个灯泡的使用寿命。所以常通过观测部分个体，以获得总体的信息。

总体根据其所包含的单位数目是否可数，可以分为有限总体和无限总体。

* 有限总体是指总体的范围能够明确确定，而且元素是有限可数的。比如，由若干个企业构成的总体就是有限总体，一批待检验的灯泡也是有限总体。
* 无限总体是指总体所包括的元素是无限的、不可数的。例如，在科学实验中，每个实验数据可以看做总体的一个元素，而实验则可以无限地进行下去，因此由实验数据构成的总体就是一个无限总体。

总体包含的观察单位通常是大量的甚至是无限的，在实际工作中，一般不可能或不必要对每个观察单位逐一进行研究。我们只能从中抽取一部分观察单位加以实际观察或调查研究，根据对这一部分观察单位的观察研究结果，再去推论和估计总体情况。

一般而言，有限总体就是有限个实数的集合。如果不是针对一批特定的灯泡，而是全面地考察某企业生产的灯泡寿命，可能的寿命是多少呢？

答案是[O,$$+\infty$$)这样一个区间。或者这样看这个问题，随机从该企业生产的灯泡中拿出一个，这个灯泡可能的寿命是多少？

答案只能是“非负实数”，当然这个“非负实数”在实际检验前是未知的。这时称该企业生产的灯泡寿命总体是取值于[O,$$+\infty$$)区间上的一个随机变量，这是一个无限总体。

在统计推断中通常是针对无限总体的，因而一般把总体看做随机变量。通常情况下，统计上的总体是一组观测数据，而不是一群人或一些物品的集合。

最后，再对总体的概念作进一步的说明。如前所述，要检验一批灯泡的寿命 ，这批灯泡构成的集合就是总体。在统计问题中，我们只是关心每个灯泡的寿命，而不是灯泡本身，所以也可以把这批灯泡的寿命集合作为总体，这个总体是一些实数构成的集合(比如10天、30天)。

### 抽样的原因：有些场合总体范围的确定则比较困难

有些总体是具体存在的，总体的范围很清楚，哪怕是无限总体。比如:
* 要检验一批灯泡的使用寿命，这批灯泡构成的集合就是总体，每个灯泡就是一个个体
* 作水质检验时从井水或河水中采的水样是样本，整个一口井或一条河所有的水,就是总体
* 临床化验中从某病人身上采的血液或其它活体组织标本，是样本；该病人全身所有的血液或某个组织器官，则是总体。

虽说我们不可能检验所有的水/所有的血液/所有的灯泡，但这类总体是具体存在的，总体的范围很容易确定。

但有些总体却是假想的，只是理论上存在的一个范围，其总体范围的确定是比较困难的。例如：
（1）对于新推出的一种饮料，要想知道消费者是否喜欢，首先必须弄清哪些人是消费的对象，也就是要确定构成该饮料的消费者这一总体，但事实上，我们很难确定哪些消费者购买该饮料，总体范围的确定十分复杂。
（2）试验某一治疗流感新药的疗效，最初接受治疗的一批流感患者，不论数量多少，都只是一个样本。若该药疗效得到肯定，从而加以推广，那么此后凡在相同条件下接受该药治疗的所有流感患者，都属于这个总体。可是当初试用时，这个总体还并不存在，是假想的。

当总体的范围难以确定时，可根据研究的目的来定义总体。

如上述某新药治疗流感例子，试验治疗的只是少数有限的病人，而结论却要推广到全体，得出一个该药对所有流感患者之疗效的规律性的认识。

所以说，观察样本的目的在于推论总体，这就是样本与总体的辩证关系。



# 
宋天龙
数据分析/数据挖掘/机器学习/智能应用
1

大数据下的数据工作是否还需要抽样？——结论，具体看情况！是否抽样看需求。

抽样是从整体样本中通过一定的方法选择一部分样本，抽样是数据处理的基本步骤之一，也是科学实验、质量检验、社会调查普遍采用的一种经济有效的工作和研究方法。

抽样工作在数据获取较少或处理大量数据比较困难的时代非常流行，主要有以下几方面背景：

数据计算资源不足。计算机软硬件的限制是导致抽样产生的基本原因之一，尤其是在数据密集的生物、科学工程等领域，不抽样往往无法对海量数据进行计算。
数据采集限制。很多时候抽样从数据采集端便已经开始，例如做社会调查必须采用抽样方法进行研究，因为根本无法做所有人群做调查。
时效性要求。抽样带来的以局部反应全局的思路，如果方法正确，可以以极小的数据计算量来实现对整体数据的统计分析，在时效性上会大大增强。
如果存在上述条件限制或有类似强制性要求，那么抽样工作仍然必不可少。但是在当前数据化运营的大背景下，数据计算资源充足、数据采集端可以采集更多的数据并且可以通过多种方式满足时效性的要求。抽样工作是否就没有必要了？其实不是的，即使上述限制条件都满足，还有很多场景依然需要通过抽样方法来解决具体问题：

通过抽样来实现快速的概念验证。数据工作中可能会包括创新性或常识性项目，对于这类项目进行快速验证、迭代和交付结论往往是概念验证的关键，通过抽样方法带来的不仅是计算效率的提升，还有前期数据准备、数据预处理、算法实现等各个方面的开发，以及服务器、硬件的配套方案的部署等内容的可行性、简单化和可操作性。
通过抽样来解决样本不均衡问题。在“3.4解决样本类别分布不均衡的问题”中，我们提到了通过欠抽样、过抽样以及组合/集成的方法解决不均衡的问题，这个过程就用到了抽样方法。
无法实现对全部样本覆盖的数据化运营场景。典型场景包括市场研究、客户线下调研分析、产品品质检验、用户电话满意度调查等，这些场景下无法实现对所有样本的采集、分析、处理和建模。
定性分析的工作需要。在定性分析工作中，通常不需要定量分析时的完整假设、精确数据和复杂统计分析过程，更多的是采用访问、观察和文献法收集资料并通过主观理解和定性分析找到问题答案，该过程中主要依靠人自身的能力而非密集的计算机能力来完成研究工作。如果不使用抽样方法，那么定性分析将很难完成。
更多，待其他答主补充。

编辑于 2017-09-01


NINGTAO WANG
一个没有理想的理想主义者
14 人赞同了该回答

必不可少！

如果你的研究目的只是获得一些描述性的统计量，比如中位数，平均值，百分比。那么在能够获得并处理总体数据的前提下，具体的抽样方法确实就没有什么意义了。

但是对于绝大多数研究来说，他们的目的在于现象背后的机理，在于因果关系的推断。在这样的意义下抽样统计是必不可少的。一般来说，对于大数据的挖掘只能得到相关性的结论，而不能得到因果关系的推断. 要得到因果推断，必须要消除其他因子对结果的潜在影响，要做到这一点就必须设计实验，设计抽样。

Correlation does not imply causation 这个链接里解释了相关性与因果关系的不同。


DataFish
数据分析师/培训师
4 人赞同了该回答
有必要。在数据和业务对接的时候，数据分析和处理并不是唯一的产能瓶颈。
例如我们要对客户做分类。客服做回访的方式来分类的话，如果做全量回访，一个月几十万用户，根本不可能做完。但如果是抽样，加上相关指标去训练模型，就有可能把待分类的客户，根据他们的行为数据来做模型。
所以抽样还是很有必要的，大数据不是为了大而大，也不是不用抽样，不用管因果，要看清背后的本质。
发布于 2014-07-25




niaocu
行危言孙。
5 人赞同了该回答
“You don’t have to eat the whole ox to know that the meat is tough（要知道牛肉很韧，毋需吃掉整头牛）.” -- Samuel Johnson
------------------------------------

首先，为什么抽样？这是来自教科书的标准解释：

节约时间（To contact the whole population would be time-consuming）。
节约成本（The cost of studying all the items in a population may be prohibitive）。
检测所有个体的实质上不可能（The physical impossibility of checking all items in the population）。比如测遍一条河里所有的鱼。
一些测试具会破坏样本（The destructive nature of some tests）。比如测试灯泡、爆竹、汽车碰撞。
样本结果足矣（The sample results are adequate）。

大数据时代或许可以节约数据收集与分析、处理的时间，但至少在以上3、4点提到的场合下，抽样仍属于必须。有时候可以认为，大数据就是在收集和计算能力提高的今天，进行一个范围更大的抽样而已。

另外，大数据的收集主要来自可观察的表面行为和现象。但对于需要深度访谈才能获取的数据（比如了解动机、想法、感受），恐怕还是得抽样。

——————

补充：从逻辑上，抽样意在归纳（从特殊到一般），大数据重在演绎（从一般到特殊），目标不同，各有存在必要。

编辑于 2019-09-03




江楚
互联网/算命的/数据/商业分析
2 人赞同了该回答
泻......药，居然是3年前的问题。数据分析其实是个很灵活的事情，什么方法好用就用什么，反正方法多工具也多。以前做抽样是因为很难做到全样本采集，要耗费巨大的人力物力时间成本，具体可参考全国人口普查，没办法才会去用抽样，从数据采集角度来说从来都是越多越详细越好。

那么现在还会用到抽样吗，会。受不同垂直行业的业务原因，不是所有公司都能拿到用户的职位，年龄，性别，收入这种偏用户画像的数据的。电视台收视率这种数据采集似乎跟以前也没什么区别吧

另外还有一种情况会用抽样，就是数据量太大，在做分析的时候，不一定要用到全量数据，这样对数据计算压力很大，反而会影响到工作效率。以前跟某家手里有好几亿用户的大公司的人聊过，他们做abtest只会抽出一小部分用户做就好，完全不需要过多数据。
发布于 2017-09-20



Mr.Wildkid
专注与大数据产品研究，读书健身，想写一本网络小说

是需要的。

选择抽样说白了两个原因：1)无法得到样本整体；2）或者得到样本整体的成本太高。

大数据时代让信息收集的成本“相对”降低，但如果你面对的情况无法解决获取样本整体的成本问题，依然需要使用抽样统计。

大数据给了我们新的思路和新的手段，但抽样统计依然是一种有智慧的、应用面很广的方法。

发布于 2017-04-11


没法粘贴：
http://www.360doc.com/content/17/0726/01/10211009_674151234.shtml